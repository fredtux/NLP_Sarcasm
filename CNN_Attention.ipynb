{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7986540,"sourceType":"datasetVersion","datasetId":4701222}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n# import tensorflow as tf\n\n# Use torch gpu\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T06:57:14.440510Z","iopub.execute_input":"2024-04-03T06:57:14.440942Z","iopub.status.idle":"2024-04-03T06:57:18.878277Z","shell.execute_reply.started":"2024-04-03T06:57:14.440906Z","shell.execute_reply":"2024-04-03T06:57:18.877285Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get data\ntrain_data = pd.read_csv('/kaggle/input/nitro2024/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nitro2024/test.csv')\n\ntrain_data['class'] = train_data['class'].replace(True, 1).replace(False, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:57:18.879826Z","iopub.execute_input":"2024-04-03T06:57:18.880223Z","iopub.status.idle":"2024-04-03T06:57:25.365614Z","shell.execute_reply.started":"2024-04-03T06:57:18.880196Z","shell.execute_reply":"2024-04-03T06:57:25.364560Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2847206600.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train_data['class'] = train_data['class'].replace(True, 1).replace(False, 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom torch import nn\nfrom torch.optim import Adam\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:57:25.366693Z","iopub.execute_input":"2024-04-03T06:57:25.366989Z","iopub.status.idle":"2024-04-03T06:57:27.694163Z","shell.execute_reply.started":"2024-04-03T06:57:25.366965Z","shell.execute_reply":"2024-04-03T06:57:27.693168Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"Alegzandra/romanian_bert-finetuned-on-REDv2-romanian\")\nmodel = AutoModel.from_pretrained(\"Alegzandra/romanian_bert-finetuned-on-REDv2-romanian\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:57:27.696590Z","iopub.execute_input":"2024-04-03T06:57:27.697461Z","iopub.status.idle":"2024-04-03T06:58:07.085658Z","shell.execute_reply.started":"2024-04-03T06:57:27.697427Z","shell.execute_reply":"2024-04-03T06:58:07.084906Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/456 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a1d97cd0d34c088332de9917f38bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/397k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a357be631542e98308812e576dc5e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.19M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b70dc2ce09e41e88738fa2131c7e1ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e552bc4784b4ad09375323ca0bd597c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0194e111404a440e85490089335c3101"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b5f2f66f9b4785a984bf936a13a8f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"new_train_data = []\nfor i in range(len(train_data)):\n    new_train_data.append((str(train_data['title'][i]) + \" \" + str(train_data[\"content\"][i])).replace(\"\\n\", \" \"))\n# for i in range(10):\n#     print(new_train_data[i], end=\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:58:07.086953Z","iopub.execute_input":"2024-04-03T06:58:07.088061Z","iopub.status.idle":"2024-04-03T06:58:08.787669Z","shell.execute_reply.started":"2024-04-03T06:58:07.088026Z","shell.execute_reply":"2024-04-03T06:58:08.786860Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nconfig = {\n    \"max_sen_len\": 512,\n    'n_gram': 1,\n    \"batch_size\": 4\n}\n\nclass CNN(nn.Module):\n    def __init__(self, alegzandra_finetuned_bert):\n        super(CNN, self).__init__()\n        self.alegzandra_finetuned_bert = alegzandra_finetuned_bert\n        # 2 CNN layers and 1 linear layer for binary classification\n        self.linear = nn.Linear(768, 2)\n        self.cnn1 = nn.Conv1d(768, 256, 2)\n        self.cnn2 = nn.Conv1d(256, 768, 2)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.phrase_attention = Phrase_attention()\n        self.self_attention = Self_Attention()\n        \n    def forward(self, p_input_ids, p_attention_mask):\n        with torch.no_grad():\n            x = self.alegzandra_finetuned_bert(input_ids=p_input_ids, attention_mask=p_attention_mask).last_hidden_state\n\n        # x = x.unsqueeze(1)\n        x = x.permute(0, 2, 1)\n        # x = x.transpose(2, 1)\n        \n        # x = x.permute(1,0,2,3)\n        \n        # print(x.shape)\n        \n        # x is input [16, 512, 768] make it [768, 16, 512]\n        # x = x.permute(2, 0, 1)\n        \n        # print('------')\n#         print('Starting CNN')\n        x = self.cnn1(x)\n        # print(x.shape)\n        x = self.relu(x)\n        x = self.dropout(x)\n        # print(x.shape)\n        # print('=============')\n        # print(x.shape)\n        \n        x = self.cnn2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        \n        x = self.maxpool(x).squeeze(2)\n\n        # print(x.shape)\n        # print('------')\n        \n        # x = x.transpose(0, 2)  \n        s_a = []\n        for i in range(len(x)):\n            # print(i)\n            text = x[i].unsqueeze(0).unsqueeze(0).permute(1, 2, 0)\n            # print(text.shape)\n            p_a = self.phrase_attention(text)\n            # print('++++++++++')\n            # print(p_a.shape)\n            # print(text.shape)\n            # print(p_a.unsqueeze(2).shape)\n            \n            s_a.append(self.self_attention(p_a * text).unsqueeze(2))\n        \n        # List of tensors to tensor\n        s_a = torch.cat(s_a, dim=2).permute(2,0,1,3).to(device)\n        # print(s_a.shape)\n        \n        x = self.maxpool(x)\n        x = self.linear(s_a)\n        # Exclude dimension 2\n        x = x.view(x.size(0), -1)\n        \n        \n        # print(x.shape)\n        # x = x.squeeze(0)\n        # x = x.transpose(0, 1)\n        return x \n    \nclass Phrase_attention(nn.Module):\n    def __init__(self):\n        super(Phrase_attention, self).__init__()\n        self.linear = nn.Linear(1, 768)\n        self.tanh = nn.Tanh()\n        self.u_w = nn.Parameter(nn.init.xavier_uniform_(torch.FloatTensor(768, 1)))\n\n    def forward(self, embedding):\n        # print(\"PA\")\n        # print(embedding.shape)\n        u_t = self.tanh(self.linear(embedding))\n        # u_t = u_t.transpose(0, 1)\n        \n        a = torch.matmul(u_t, self.u_w)\n        # print('++++++++++')\n        \n        # print(a.shape)\n        a = a.squeeze(2)\n        # print('++++++++++')\n        \n        \n        a = F.log_softmax(a, dim=1)\n        \n        return a\n\nclass Self_Attention(nn.Module):\n    def __init__(self):\n        super(Self_Attention, self).__init__()\n        self.w1 = nn.Parameter(nn.init.xavier_uniform_(torch.FloatTensor(768, 1)))\n        \n        self.w2 = nn.Parameter(nn.init.xavier_uniform_(torch.FloatTensor(768, 1)))\n        \n        self.b = nn.Parameter(torch.FloatTensor(torch.randn(1)))\n\n    def forward(self, embedding):\n        f1 = torch.matmul(embedding, self.w1)\n\n        # print('////////////////////')\n        # print(f1.shape)        \n        f2 = torch.matmul(embedding, self.w2)\n        # print(f2.shape)\n        \n        f1 = f1.repeat(1, 1, embedding.size(1))\n        # print(f1.shape)\n        \n        f2 = f2.repeat(1, 1, embedding.size(1))\n        # print(f2.shape)\n        \n\n        f1 = f1\n        S = f1 + f2 + self.b\n        # S = S + self.b\n        \n        mask = torch.eye(768).type(torch.ByteTensor)\n        # print('111')\n        # print(S.shape)\n        # print(S.shape)\n        S = S.transpose(0,1)\n        # print(embedding.shape)\n        # print(S.shape)\n        S = S.masked_fill(mask.bool().to(device), -float('inf'))\n        # print('222')\n        \n        # print(S.shape)\n        # print('333')\n        max_row = F.max_pool1d(S, kernel_size=embedding.size(1), stride=1)\n        # max_row.transpose(1, 2)\n        # print(max_row.shape)\n        a = F.softmax(max_row, dim=1).transpose(1, 2)\n        # print(a.shape)\n        embedding = embedding.squeeze(0)\n        # print(embedding.shape)\n        \n        v_a = torch.matmul(a, embedding)\n\n        return v_a\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:58:08.789066Z","iopub.execute_input":"2024-04-03T06:58:08.789508Z","iopub.status.idle":"2024-04-03T06:58:08.815548Z","shell.execute_reply.started":"2024-04-03T06:58:08.789474Z","shell.execute_reply":"2024-04-03T06:58:08.814576Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score\nbatch_size = 4\n\nalegzandra_finetuned_bert = AutoModel.from_pretrained(\"Alegzandra/romanian_bert-finetuned-on-REDv2-romanian\", output_hidden_states = True, num_labels=512).to(device)\n\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(train_data['class'])\n\nmodel = CNN(alegzandra_finetuned_bert).to(device)\n\nX_train, X_val, y_train, y_val = train_test_split(new_train_data, y, test_size=0.2, random_state=42)\n\nloss_fn = nn.CrossEntropyLoss()\nmodel.alegzandra_finetuned_bert.requires_grad_(False)\nparams = [param for param in model.parameters() if param.requires_grad]\noptimizer = Adam(params, lr=0.0001)\n\nlosses = []\nfor epoch in range(1):\n    for i in range(0, len(new_train_data), batch_size):\n        model.train()\n        model.alegzandra_finetuned_bert.eval()\n        if i % 1000 == 0:\n            print(i)\n        X_train = tokenizer(new_train_data[i:i+batch_size], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n        # print(X_train['input_ids'].shape)\n        y_train = torch.tensor(y[i:i+batch_size], dtype=torch.int32).long().to(device)\n        # print(X_train, y[i])\n\n        p_input_ids = X_train['input_ids'].to(device)\n        p_attention_mask = X_train['attention_mask'].to(device)\n\n        # test = alegzandra_finetuned_bert(input_ids=p_input_ids, attention_mask=p_attention_mask).last_hidden_state\n        # print(test.shape)\n\n        res = model(p_input_ids, p_attention_mask)\n        # res = torch.argmax(res).to(device)\n\n        loss = loss_fn(res, y_train)\n        if i % 1000 == 0:\n            print(loss)\n            torch.save(model.state_dict(), '/kaggle/working/model.pth')\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        # break\n        \n        if i + batch_size > len(new_train_data):\n            torch.save(model.state_dict(), '/kaggle/working/model.pth')\n#             break\n\n\n        # print(res[0].shape)\n        # Validation\n    model.eval()\n    model.alegzandra_finetuned_bert.eval()\n    with torch.no_grad():\n        for i in range(0, len(X_val), batch_size):\n            X_val_test = tokenizer(X_val[i:i+batch_size], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n            y_val_test = torch.tensor(y_val[i:i+batch_size], dtype=torch.int32).long().to(device)\n\n            p_input_ids = X_val_test['input_ids'].to(device)\n            p_attention_mask = X_val_test['attention_mask'].to(device)\n\n            res = model(p_input_ids, p_attention_mask)\n            probabilities = torch.sigmoid(res)\n            res = torch.argmax(probabilities, dim=-1).to(device)\n            # if res < 0.5:\n            #     res = 0\n            # else:\n            #     res = 1\n\n            # # Balanced accuracy\n            balanced_accuracy = balanced_accuracy_score(y_val_test.cpu(), res.cpu())\n            \n            if i % 500 == 0:\n                print(balanced_accuracy)\n            break\n            \n#             if i + batch_size > len(X_val):\n#                 break\n            \ntorch.save(model.state_dict(), '/kaggle/working/model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:58:08.816789Z","iopub.execute_input":"2024-04-03T06:58:08.817065Z","iopub.status.idle":"2024-04-03T10:41:15.630893Z","shell.execute_reply.started":"2024-04-03T06:58:08.817032Z","shell.execute_reply":"2024-04-03T10:41:15.629922Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"0\ntensor(12.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n1000\ntensor(7.0765, device='cuda:0', grad_fn=<NllLossBackward0>)\n2000\ntensor(6.9895, device='cuda:0', grad_fn=<NllLossBackward0>)\n3000\ntensor(7.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n4000\ntensor(6.8280, device='cuda:0', grad_fn=<NllLossBackward0>)\n5000\ntensor(6.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n6000\ntensor(6.8117, device='cuda:0', grad_fn=<NllLossBackward0>)\n7000\ntensor(6.8547, device='cuda:0', grad_fn=<NllLossBackward0>)\n8000\ntensor(6.8030, device='cuda:0', grad_fn=<NllLossBackward0>)\n9000\ntensor(7.3061, device='cuda:0', grad_fn=<NllLossBackward0>)\n10000\ntensor(6.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n11000\ntensor(6.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n12000\ntensor(6.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n13000\ntensor(6.8447, device='cuda:0', grad_fn=<NllLossBackward0>)\n14000\ntensor(6.8983, device='cuda:0', grad_fn=<NllLossBackward0>)\n15000\ntensor(6.7707, device='cuda:0', grad_fn=<NllLossBackward0>)\n16000\ntensor(6.8863, device='cuda:0', grad_fn=<NllLossBackward0>)\n17000\ntensor(6.8821, device='cuda:0', grad_fn=<NllLossBackward0>)\n18000\ntensor(7.1824, device='cuda:0', grad_fn=<NllLossBackward0>)\n19000\ntensor(7.1231, device='cuda:0', grad_fn=<NllLossBackward0>)\n20000\ntensor(6.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n21000\ntensor(6.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n22000\ntensor(6.9583, device='cuda:0', grad_fn=<NllLossBackward0>)\n23000\ntensor(6.7463, device='cuda:0', grad_fn=<NllLossBackward0>)\n24000\ntensor(6.9231, device='cuda:0', grad_fn=<NllLossBackward0>)\n25000\ntensor(6.8447, device='cuda:0', grad_fn=<NllLossBackward0>)\n26000\ntensor(6.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n27000\ntensor(6.8249, device='cuda:0', grad_fn=<NllLossBackward0>)\n28000\ntensor(6.7348, device='cuda:0', grad_fn=<NllLossBackward0>)\n29000\ntensor(6.7024, device='cuda:0', grad_fn=<NllLossBackward0>)\n30000\ntensor(6.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n31000\ntensor(6.8218, device='cuda:0', grad_fn=<NllLossBackward0>)\n32000\ntensor(6.8148, device='cuda:0', grad_fn=<NllLossBackward0>)\n33000\ntensor(6.7441, device='cuda:0', grad_fn=<NllLossBackward0>)\n34000\ntensor(6.8091, device='cuda:0', grad_fn=<NllLossBackward0>)\n35000\ntensor(6.8007, device='cuda:0', grad_fn=<NllLossBackward0>)\n36000\ntensor(6.8891, device='cuda:0', grad_fn=<NllLossBackward0>)\n37000\ntensor(6.7176, device='cuda:0', grad_fn=<NllLossBackward0>)\n38000\ntensor(6.7224, device='cuda:0', grad_fn=<NllLossBackward0>)\n39000\ntensor(7.0472, device='cuda:0', grad_fn=<NllLossBackward0>)\n40000\ntensor(6.8498, device='cuda:0', grad_fn=<NllLossBackward0>)\n41000\ntensor(6.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n42000\ntensor(6.7979, device='cuda:0', grad_fn=<NllLossBackward0>)\n43000\ntensor(6.9577, device='cuda:0', grad_fn=<NllLossBackward0>)\n44000\ntensor(6.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n45000\ntensor(6.7673, device='cuda:0', grad_fn=<NllLossBackward0>)\n46000\ntensor(6.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n47000\ntensor(6.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n48000\ntensor(6.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n49000\ntensor(6.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n50000\ntensor(6.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n51000\ntensor(6.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n52000\ntensor(6.8316, device='cuda:0', grad_fn=<NllLossBackward0>)\n53000\ntensor(6.7591, device='cuda:0', grad_fn=<NllLossBackward0>)\n54000\ntensor(6.8105, device='cuda:0', grad_fn=<NllLossBackward0>)\n55000\ntensor(6.9751, device='cuda:0', grad_fn=<NllLossBackward0>)\n56000\ntensor(6.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n57000\ntensor(6.7882, device='cuda:0', grad_fn=<NllLossBackward0>)\n58000\ntensor(6.7545, device='cuda:0', grad_fn=<NllLossBackward0>)\n59000\ntensor(6.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n60000\ntensor(6.7465, device='cuda:0', grad_fn=<NllLossBackward0>)\n61000\ntensor(6.8620, device='cuda:0', grad_fn=<NllLossBackward0>)\n62000\ntensor(7.1395, device='cuda:0', grad_fn=<NllLossBackward0>)\n63000\ntensor(6.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n64000\ntensor(6.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n65000\ntensor(6.7273, device='cuda:0', grad_fn=<NllLossBackward0>)\n66000\ntensor(6.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n67000\ntensor(6.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n68000\ntensor(6.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n69000\ntensor(6.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n70000\ntensor(6.7237, device='cuda:0', grad_fn=<NllLossBackward0>)\n0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"from pandas import DataFrame\n\n# Create data frame with id and class header\ndf = DataFrame(test_data['id'], columns=['id'])\n\nnew_test_data = []\nfor i in range(len(test_data)):\n    new_test_data.append((str(test_data['title'][i]) + \" \" + str(test_data[\"content\"][i])).replace(\"\\n\", \" \"))\n\nwith torch.no_grad():\n    model.eval()\n    for i in range(0, len(test_data), batch_size):\n        X_test = tokenizer(new_test_data[i:i+batch_size], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n        p_input_ids = X_test['input_ids'].to(device)\n        p_attention_mask = X_test['attention_mask'].to(device)\n\n        res = model(p_input_ids, p_attention_mask)\n        probabilities = torch.sigmoid(res)\n        res = torch.argmax(probabilities, dim=-1).to(device)\n        \n        # Add all rows from res to df\n        for j in range(len(res)):\n            df.loc[i+j, 'class'] = int(res[j].item())\n        \n        \n        \ndf.to_csv('/kaggle/working/submission.csv', index=False)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-03T10:41:15.632100Z","iopub.execute_input":"2024-04-03T10:41:15.632571Z"},"trusted":true},"execution_count":null,"outputs":[]}]}